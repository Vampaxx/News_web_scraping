{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "#os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Asus\\\\vs_code\\\\Internship\\\\News_web_scraping'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class WebScrapingConfig:\n",
    "    headers         : str \n",
    "    extracted_path  : Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from News_web_scraping import logger\n",
    "from News_web_scraping.constants import *\n",
    "from News_web_scraping.utils.common import *\n",
    "from News_web_scraping.entity.config_entity import (ModelConfig,\n",
    "                                                      WebScrapingConfig)\n",
    "\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath    = CONFIG_FILE_PATH,\n",
    "                 params_filepath    = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "\n",
    "    def get_model_config(self) -> ModelConfig:\n",
    "        params  = self.params\n",
    "        logger.info(\"Model config initialized\")\n",
    "        load_dotenv()\n",
    "        model_config = ModelConfig(Model_name   = params.MODEL_NAME,\n",
    "                                   temperature  = params.TEMPERATURE,\n",
    "                                   api_key      = os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "        return model_config\n",
    "    \n",
    "\n",
    "    def get_web_scraping_config(self) -> WebScrapingConfig:\n",
    "        config              = self.config\n",
    "        web_scraping_config = self.config.Web_scraping\n",
    "\n",
    "        logger.info(\"web scraping initialized\")\n",
    "        web_scraping_config = WebScrapingConfig(headers         = config.headers,\n",
    "                                                extracted_path  = web_scraping_config.extracted_path)\n",
    "        return web_scraping_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-04 13:32:03,123: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-04 13:32:03,123: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-04 13:32:03,125: INFO: 1393433061: web scraping initialized]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConfigBox({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:130.0) Gecko/20100101 Firefox/130.0', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8', 'DNT': '1', 'Connection': 'close', 'Upgrade-Insecure-Requests': '1'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager             = ConfigurationManager()\n",
    "web_scraping_config = manager.get_web_scraping_config() \n",
    "web_scraping_config.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "class WebPageScraper(ConfigurationManager):\n",
    "\n",
    "    def __init__(self,URL):\n",
    "        super().__init__()\n",
    "        self.URL            = URL \n",
    "        self.soup           = None\n",
    "        web_scraping_config = self.get_web_scraping_config()\n",
    "        self.headers        = web_scraping_config.headers\n",
    "\n",
    "\n",
    "    def fetch_and_parse(self): \n",
    "        \"\"\"Validate the URL, fetch the webpage, and parse the content using BeautifulSoup.\"\"\"\n",
    "        parsed_url = urlparse(self.URL)\n",
    "        if not parsed_url.scheme:\n",
    "            self.URL = f\"http://{self.URL}\"  \n",
    "        \n",
    "        try:\n",
    "            response    = requests.get(self.URL, headers=self.headers)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            self.soup   = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        except requests.exceptions.MissingSchema:\n",
    "            logger.error(f\"Invalid URL: {self.URL} - Missing schema.\")\n",
    "            raise ValueError(f\"Invalid URL: {self.URL} - Please provide a valid URL.\")\n",
    "        \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            logger.error(f\"Connection error while trying to reach {self.URL}.\")\n",
    "            raise ConnectionError(f\"Failed to connect to {self.URL}.\")\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.error(f\"Timeout error while trying to reach {self.URL}.\")\n",
    "            raise TimeoutError(f\"Request to {self.URL} timed out.\")\n",
    "        \n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            logger.error(f\"HTTP error occurred: {err}\")\n",
    "            raise RuntimeError(f\"HTTP error occurred: {err}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred: {e}\")\n",
    "            raise RuntimeError(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "    def clean_and_extract_text(self):\n",
    "        \"\"\"Remove unwanted tags and extract text from paragraph (<p>) tags.\"\"\"\n",
    "        if self.soup:\n",
    "            for unwanted in self.soup([\"script\", \"style\", \"a\"]):\n",
    "                unwanted.extract()\n",
    "            paragraphs = self.soup.find_all('p')\n",
    "            if paragraphs:\n",
    "                return ' '.join(para.get_text(separator=' ', strip=True) for para in paragraphs)\n",
    "            else:\n",
    "                logger.warning(\"No paragraph tags found in the response.\")\n",
    "                return \"No content found.\"\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL = \"https://medium.com/@hamedkazemi/breaking-the-token-limits-a-journey-with-chatgpt-langchain-and-vectordb-embeddings-bonus-32352f97f5d4\"\n",
    "#URL = \"https://indianexpress.com/article/explained/explained-global/why-chagos-islands-matter-why-uk-keeps-diego-garcia-base-9602682/?ref=newlist_hp\"\n",
    "#URL = \"https://indianexpress.com/article/explained/explained-global/why-chagos-islands-matter-why-uk-keeps-diego-garcia-base-9602682/?ref=newlist_hp\"\n",
    "URL = \"https://www.gsmarena.com/detailed_specs_for_samsungs_galaxy_a16_4g_and_5g_emerge-news-64767.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-04 13:32:03,172: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-04 13:32:03,172: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-04 13:32:03,172: INFO: 1393433061: web scraping initialized]\n"
     ]
    }
   ],
   "source": [
    "scraper     = WebPageScraper(URL=URL)\n",
    "scraper.fetch_and_parse()\n",
    "page_text   = scraper.clean_and_extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Login 02 October 2024  Samsung is expected to announce its entry-level Galaxy A16 in 4G and 5G versions in and we now have detailed spec sheets for both. The latest round of leaks comes on the heels of the and leaks and confirms everything we can expect to see from the two upcoming A-series phones. Galaxy A16 5G key specs and render As their names suggest, and are nearly the exact same phone with the major difference being their chipsets. In Europe where the new listings are from, A16 5G will be powered by Samsung’s own Exynos 1330 SoC. Past suggest other regions like South East Asia will see the A16 5G with MediaTek’s Dimensity 6300 at the helm. The A15 4G on the other hand will bring MediaTek’s old Helio G99 just like its . Galaxy A16 4G key specs and render The rest of the specs for both Galaxy A16 variants are identical. A 6.7-inch Super AMOLED display (FHD+ 90Hz), 50MP main cam, joined by 5MP ultrawide and 2MP macro modules, and a 5,000 mAh battery with 25W charging. Both phones will have 4GB RAM and 128GB storage, supporting memory card expansion via microSD. Samsung is also to offer up to 6 years of Android updates and security patches. Galaxy A16 4G should cost around €210 while the Galaxy A16 5G will start at €240 in Europe.  The name So in other words 5G isn't really a thing in almost any country that isn't first world... Yeah A15 is forever the goat. Even the company is repeating and milking It's success, just like how they did it with the A12/A12s. A13 is basically a repeat of the A12s.    © 2000-2024\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from News_web_scraping.pipeline.stage_02_prompt_and_chain import PromptAndChainPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-04 13:32:04,715: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-04 13:32:04,718: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-04 13:32:04,718: INFO: common: created directory at: artifacts]\n",
      "[2024-10-04 13:32:04,719: INFO: configuration: Model config initialized]\n",
      "[2024-10-04 13:32:04,721: INFO: Model: Model setup initialized]\n",
      "[2024-10-04 13:32:05,248: INFO: Model: model----llama3-8b-8192----created]\n",
      "[2024-10-04 13:32:05,248: INFO: prompt_and_chain: Prompting and chain has started]\n"
     ]
    }
   ],
   "source": [
    "obj     = PromptAndChainPipeline()\n",
    "llm     = obj.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-04 13:32:05,948: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"]\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from requests.exceptions import RequestException, MissingSchema, ConnectionError, Timeout, HTTPError\n",
    "\n",
    "import json\n",
    "from json.decoder import JSONDecodeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombiningPipeline:\n",
    "    def __init__(self, URL):\n",
    "        self.URL = URL\n",
    "    \n",
    "    def main(self):\n",
    "        try:\n",
    "            # Initialize the scraper and fetch the webpage content\n",
    "            scraper = WebPageScraper(URL=self.URL)\n",
    "            scraper.fetch_and_parse()\n",
    "            page_text = scraper.clean_and_extract_text()\n",
    "\n",
    "            # Handle the case where the content is 'No content found.'\n",
    "            if page_text == \"No content found.\":\n",
    "                logger.error(\"No content found on the webpage. Exiting process.\")\n",
    "                raise ValueError(\"The webpage did not contain any extractable content.\")\n",
    "\n",
    "            # Handle the case where extracted text is empty or only whitespace\n",
    "            if not page_text or page_text.strip() == \"\":\n",
    "                logger.error(\"The extracted content is empty or contains only whitespace.\")\n",
    "                raise OutputParserException(\"No data to extract from the provided webpage.\")\n",
    "\n",
    "            # Process the extracted text using the PromptAndChainPipeline\n",
    "            prompt = PromptAndChainPipeline()\n",
    "            chain = prompt.main()\n",
    "\n",
    "            # Invoke the chain with the extracted page text\n",
    "            return chain.invoke(page_text)\n",
    "        \n",
    "        # Handle output parsing errors if the output is invalid or empty\n",
    "        except OutputParserException as e:\n",
    "            logger.error(f\"OutputParserException: {e}\")\n",
    "            return {\n",
    "                \"error\": \"Output parsing failed.\",\n",
    "                \"message\": str(e),\n",
    "                \"suggestion\": \"Ensure the webpage contains valid content for extraction.\"\n",
    "            }\n",
    "\n",
    "        # Handle invalid JSON outputs during the parsing phase\n",
    "        except JSONDecodeError as e:\n",
    "            logger.error(f\"JSONDecodeError: {e}\")\n",
    "            return {\n",
    "                \"error\": \"Invalid JSON output.\",\n",
    "                \"message\": \"The output from the model was not valid JSON.\",\n",
    "                \"suggestion\": \"Check the model's output format and ensure it is correct.\"\n",
    "            }\n",
    "\n",
    "        # Handle requests-related issues such as timeouts, connection errors, etc.\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"RequestException while fetching the webpage: {e}\")\n",
    "            return {\n",
    "                \"error\": \"Failed to fetch the webpage.\",\n",
    "                \"message\": str(e),\n",
    "                \"suggestion\": \"Check the URL, internet connection, or server status.\"\n",
    "            }\n",
    "\n",
    "        # Handle cases where no content is found\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"ValueError: {e}\")\n",
    "            return {\n",
    "                \"error\": \"No content found.\",\n",
    "                \"message\": str(e),\n",
    "                \"suggestion\": \"Ensure the webpage has relevant content.\"\n",
    "            }\n",
    "\n",
    "        # General exception handling for any unexpected errors\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred: {e}\")\n",
    "            return {\n",
    "                \"error\": \"An unexpected error occurred.\",\n",
    "                \"message\": str(e),\n",
    "                \"suggestion\": \"Check the logs for more details or contact support.\"\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-04 13:32:06,086: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-04 13:32:06,088: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-04 13:32:06,089: INFO: 1393433061: web scraping initialized]\n",
      "[2024-10-04 13:32:06,977: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-04 13:32:06,977: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-04 13:32:06,979: INFO: common: created directory at: artifacts]\n",
      "[2024-10-04 13:32:06,979: INFO: configuration: Model config initialized]\n",
      "[2024-10-04 13:32:06,982: INFO: Model: Model setup initialized]\n",
      "[2024-10-04 13:32:07,352: INFO: Model: model----llama3-8b-8192----created]\n",
      "[2024-10-04 13:32:07,352: INFO: prompt_and_chain: Prompting and chain has started]\n",
      "[2024-10-04 13:32:08,021: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"]\n"
     ]
    }
   ],
   "source": [
    "respone = CombiningPipeline(URL=URL).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://python.langchain.com/docs/integrations/chat/groq/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-04 13:35:19,657: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-04 13:35:19,661: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-04 13:35:19,661: INFO: 1393433061: web scraping initialized]\n",
      "[2024-10-04 13:35:20,071: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-04 13:35:20,071: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-04 13:35:20,071: INFO: common: created directory at: artifacts]\n",
      "[2024-10-04 13:35:20,076: INFO: configuration: Model config initialized]\n",
      "[2024-10-04 13:35:20,076: INFO: Model: Model setup initialized]\n",
      "[2024-10-04 13:35:20,431: INFO: Model: model----llama3-8b-8192----created]\n",
      "[2024-10-04 13:35:20,432: INFO: prompt_and_chain: Prompting and chain has started]\n",
      "[2024-10-04 13:35:20,973: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"]\n"
     ]
    }
   ],
   "source": [
    "pipeline        = CombiningPipeline(URL=URL)\n",
    "response        = pipeline.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Galaxy A16 Models': {'A16 5G': {'Chipset': 'Samsung Exynos 1330 (Europe) / MediaTek Dimensity 6300 (other regions)',\n",
       "   'Display': '6.7-inch Super AMOLED (FHD+ 90Hz)',\n",
       "   'Camera': '50MP main cam, 5MP ultrawide, 2MP macro',\n",
       "   'Battery': '5,000 mAh with 25W charging',\n",
       "   'RAM': '4GB',\n",
       "   'Storage': '128GB (expandable via microSD)',\n",
       "   'Android Updates': 'Up to 6 years',\n",
       "   'Price': '€240 (Europe)'},\n",
       "  'A16 4G': {'Chipset': 'MediaTek Helio G99',\n",
       "   'Display': '6.7-inch Super AMOLED (FHD+ 90Hz)',\n",
       "   'Camera': '50MP main cam, 5MP ultrawide, 2MP macro',\n",
       "   'Battery': '5,000 mAh with 25W charging',\n",
       "   'RAM': '4GB',\n",
       "   'Storage': '128GB (expandable via microSD)',\n",
       "   'Android Updates': 'Up to 6 years',\n",
       "   'Price': '€210 (Europe)'}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['Galaxy A16 Models']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
